{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def detect_human(image_path):\n",
    "    # Load YOLOv3 model files\n",
    "    config_path = r'D:\\University\\Computer Vision\\Project\\yolov3.cfg'\n",
    "    weights_path = r'D:\\University\\Computer Vision\\Project\\yolov3.weights'\n",
    "    class_path = r'D:\\University\\Computer Vision\\Project\\coco.names'\n",
    "\n",
    "    # Load the network and classes\n",
    "    net = cv2.dnn.readNetFromDarknet(config_path, weights_path)\n",
    "    classes = []\n",
    "    with open(class_path, 'r') as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    # Perform blob preprocessing\n",
    "    blob = cv2.dnn.blobFromImage(image, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
    "\n",
    "    # Set the input blob for the network\n",
    "    net.setInput(blob)\n",
    "\n",
    "    # Run forward pass and get output layers\n",
    "    layer_names = net.getLayerNames()\n",
    "    output_layers_names = net.getUnconnectedOutLayersNames()\n",
    "    outputs = net.forward(output_layers_names)\n",
    "\n",
    "    # Initialize lists for bounding boxes, confidences, and class IDs\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "\n",
    "    # Process each output layer\n",
    "    for output in outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.5 and class_id == 0:  # Assuming class 0 represents humans\n",
    "                # Scale the bounding box coordinates\n",
    "                box = detection[0:4] * np.array([width, height, width, height])\n",
    "                (center_x, center_y, bbox_width, bbox_height) = box.astype(\"int\")\n",
    "\n",
    "                # Calculate top-left corner coordinates\n",
    "                x = int(center_x - (bbox_width / 2))\n",
    "                y = int(center_y - (bbox_height / 2))\n",
    "\n",
    "                # Append the bounding box information\n",
    "                boxes.append([x, y, int(bbox_width), int(bbox_height)])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    # Apply non-maximum suppression to remove redundant overlapping boxes\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, score_threshold=0.5, nms_threshold=0.3)\n",
    "\n",
    "    # Draw bounding boxes and labels for the detected humans\n",
    "    for i in indices:\n",
    "        i = i\n",
    "        x, y, w, h = boxes[i]\n",
    "        label = f\"Human: {confidences[i]:.2f}\"\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cv2.putText(image, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the image with detections\n",
    "    cv2.imshow('Human Detection', image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "image_path = r'D:\\University\\Computer Vision\\Project\\800px-Europei_di_pallavolo_2005_-_Italia-Russia.jpg'\n",
    "detect_human(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def detect_human_video(frame_rate):\n",
    "    config_path = r'D:\\University\\Computer Vision\\Project\\yolov3.cfg'\n",
    "    weights_path = r'D:\\University\\Computer Vision\\Project\\yolov3.weights'\n",
    "    class_path = r'D:\\University\\Computer Vision\\Project\\coco.names'\n",
    "\n",
    "    net = cv2.dnn.readNetFromDarknet(config_path, weights_path)\n",
    "    classes = []\n",
    "    with open(class_path, 'r') as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    cap = cv2.VideoCapture(r'D:\\University\\Computer Vision\\Project\\2.mp4')   #r'D:\\University\\Computer Vision\\Project\\1.mp4'\n",
    "\n",
    "    actual_frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    delay = int(1000 / frame_rate)\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "    boxes = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        frame = cv2.resize(frame, (1600,800), interpolation = cv2.INTER_AREA)\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        height, width, _ = frame.shape\n",
    "\n",
    "        blob = cv2.dnn.blobFromImage(frame, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
    "\n",
    "        net.setInput(blob)\n",
    "\n",
    "        layer_names = net.getLayerNames()\n",
    "        output_layers_names = net.getUnconnectedOutLayersNames()\n",
    "        outputs = net.forward(output_layers_names)\n",
    "\n",
    "        for output in outputs:\n",
    "            for detection in output:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "\n",
    "                if confidence > 0.8 and class_id == 0:  \n",
    "                    box = detection[0:4] * np.array([width, height, width, height])\n",
    "                    (x, y, w, h) = box.astype(\"int\")\n",
    "                    boxes.append([x, y, w, h])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "                    cv2.rectangle(frame, (x- int(w/2), y- int(h/2)), (x + int(w/2), y + int(h/2)), (0, 255, 0), 2)\n",
    "                    label = f\"Human: {confidence:.2f}\"\n",
    "                  #  cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        # indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    \n",
    "\n",
    "        # for i in range(len(boxes)):\n",
    "        #     if i in indexes:\n",
    "        #         x, y, w, h = boxes[i]\n",
    "        #         roi = frame[y- int(h/2) : y + int(h/2) , x- int(w/2) : x + int(w/2)]  # Extract the ROI\n",
    "                \n",
    "        #         # Save the ROI image\n",
    "        #         cv2.imwrite(r'D:\\University\\Computer Vision\\Project\\Datasets\\All\\human_'+str(i+10940)+'.jpg', roi)\n",
    "                \n",
    "\n",
    "        cv2.imshow('Human Detection', frame)\n",
    "        time.sleep(1)\n",
    "        key = cv2.waitKey(delay)\n",
    "\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "frame_rate = 1\n",
    "\n",
    "detect_human_video(frame_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classiacal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import openpose\n",
    "\n",
    "# Load OpenPose model\n",
    "net = cv2.dnn.readNetFromTensorflow(\"pose/coco/pose_iter_440000.caffemodel\", \"pose/coco/pose_deploy_linevec.prototxt\")\n",
    "\n",
    "# Set input size\n",
    "width, height = 368, 368\n",
    "\n",
    "# Load input image\n",
    "image = cv2.imread(\"input_image.jpg\")\n",
    "image_height, image_width, _ = image.shape\n",
    "\n",
    "# Resize image to input size\n",
    "input_image = cv2.resize(image, (width, height))\n",
    "\n",
    "# Create a blob from the image\n",
    "blob = cv2.dnn.blobFromImage(input_image, 1.0, (width, height), (127.5, 127.5, 127.5), swapRB=True, crop=False)\n",
    "\n",
    "# Set the blob as input to the network\n",
    "net.setInput(blob)\n",
    "\n",
    "# Forward pass through the network\n",
    "output = net.forward()\n",
    "\n",
    "# Get the points and lines for each body part\n",
    "points = []\n",
    "lines = []\n",
    "for i in range(len(openpose.BODY_PARTS)):\n",
    "    # Get confidence map for body part\n",
    "    prob_map = output[0, i, :, :]\n",
    "\n",
    "    # Find global maxima of the prob_map\n",
    "    _, confidence, _, point = cv2.minMaxLoc(prob_map)\n",
    "\n",
    "    # Scale the point to the size of the input image\n",
    "    x = int((image_width * point[0]) / width)\n",
    "    y = int((image_height * point[1]) / height)\n",
    "\n",
    "    # Add the point to the list\n",
    "    points.append((x, y))\n",
    "\n",
    "    # Add line indices if available\n",
    "    if i in openpose.POSE_PAIRS:\n",
    "        line = openpose.POSE_PAIRS[i]\n",
    "        lines.append([points[line[0]], points[line[1]]])\n",
    "\n",
    "# Draw points and lines on the image\n",
    "for point in points:\n",
    "    cv2.circle(image, point, 5, (0, 255, 0), -1)\n",
    "\n",
    "for line in lines:\n",
    "    cv2.line(image, line[0], line[1], (0, 0, 255), 3)\n",
    "\n",
    "# Display the output image\n",
    "cv2.imshow(\"Output\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load input image\n",
    "image = cv2.imread(r\"D:\\University\\Computer Vision\\Project\\Datasets\\All\\human_1353.jpg\")\n",
    "\n",
    "# Convert the image to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply Gaussian blur to reduce noise\n",
    "blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "# Perform edge detection\n",
    "edges = cv2.Canny(blur, 120, 130)\n",
    "\n",
    "# Find contours in the edge map\n",
    "contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Sort contours by area in descending order\n",
    "contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "\n",
    "# Iterate over contours\n",
    "for contour in contours:\n",
    "    # Calculate the perimeter of the contour\n",
    "    perimeter = cv2.arcLength(contour, True)\n",
    "\n",
    "    # Approximate the contour as a polygon\n",
    "    epsilon = 0.02 * perimeter\n",
    "    approx = cv2.approxPolyDP(contour, epsilon, True)\n",
    "\n",
    "    # If the contour has 4 points, it could be a human pose\n",
    "    if len(approx) == 4:\n",
    "        # Draw the contour on the image\n",
    "        cv2.drawContours(image, [approx], -1, (0, 255, 0), 3)\n",
    "        break  # Stop iterating over contours after finding the pose\n",
    "\n",
    "# Display the output image\n",
    "cv2.imshow(\"Output\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\caffe\\caffe_io.cpp:1126: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"path/to/pose_deploy_linevec.prototxt\" in function 'cv::dnn::ReadProtoFromTextFile'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14892\\1322937390.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Load the pre-trained model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadNetFromCaffe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'path/to/pose_deploy_linevec.prototxt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'path/to/pose_iter_XXXX.caffemodel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Define the body part connections\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\caffe\\caffe_io.cpp:1126: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"path/to/pose_deploy_linevec.prototxt\" in function 'cv::dnn::ReadProtoFromTextFile'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained model\n",
    "net = cv2.dnn.readNetFromCaffe('path/to/pose_deploy_linevec.prototxt', 'path/to/pose_iter_XXXX.caffemodel')\n",
    "\n",
    "# Define the body part connections\n",
    "body_parts_connections = [\n",
    "    # Torso\n",
    "    [0, 1], [1, 2], [2, 3], [3, 4],  # Right side\n",
    "    [1, 5], [5, 6], [6, 7], [1, 8],  # Left side\n",
    "    [8, 9], [9, 10], [1, 11], [11, 12],  # Hips\n",
    "    [12, 13],  # Right leg\n",
    "    [13, 14], [14, 15],  # Right foot\n",
    "    [11, 16], [16, 17],  # Left leg\n",
    "    [17, 18], [18, 19],  # Left foot\n",
    "    # Arms\n",
    "    [2, 16], [3, 15], [4, 14],  # Right arm\n",
    "    [5, 11], [6, 12], [7, 13]  # Left arm\n",
    "]\n",
    "\n",
    "# Open the webcam or video stream\n",
    "cap = cv2.VideoCapture(0)  # Change the argument to a video file path if you want to use a video file\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video stream\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Resize the frame to a suitable size for processing\n",
    "    width, height = 368, 368\n",
    "    input_blob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (width, height), (0, 0, 0), swapRB=True, crop=False)\n",
    "\n",
    "    # Set the input blob as the network input\n",
    "    net.setInput(input_blob)\n",
    "\n",
    "    # Forward pass through the network\n",
    "    output = net.forward()\n",
    "\n",
    "    # Extract the detected keypoints\n",
    "    num_points = output.shape[2]\n",
    "    keypoints = []\n",
    "    for i in range(num_points):\n",
    "        # Confidence map of corresponding body's part\n",
    "        prob_map = output[0, i, :, :]\n",
    "        # Find the location of the body part with the highest confidence\n",
    "        _, confidence, _, point = cv2.minMaxLoc(prob_map)\n",
    "        # Scale the point back to the original frame size\n",
    "        x = int((frame.shape[1] * point[0]) / width)\n",
    "        y = int((frame.shape[0] * point[1]) / height)\n",
    "        # Add the keypoint to the list\n",
    "        keypoints.append((x, y) if confidence > 0.1 else None)\n",
    "\n",
    "    # Draw the detected keypoints on the frame\n",
    "    for keypoint in keypoints:\n",
    "        if keypoint is not None:\n",
    "            cv2.circle(frame, keypoint, 4, (0, 255, 0), -1)\n",
    "\n",
    "    # Draw the body part connections\n",
    "    for i, (a, b) in enumerate(body_parts_connections):\n",
    "        if keypoints[a] is not None and keypoints[b] is not None:\n",
    "            cv2.line(frame, keypoints[a], keypoints[b], (0, 255, 255), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Real-time Pose Estimation\", frame)\n",
    "\n",
    "    # Break the loop if the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close any open windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14892\\2319943348.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# Detect people in the frame using HOG\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mboxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwinStride\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# Draw bounding boxes around the detected people\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained HOG model for human detection\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "# Open a video capture object (0 for webcam)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Read the frame from the video capture\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Resize the frame to a reasonable size for faster processing\n",
    "    frame = cv2.resize(frame, (640, 480))\n",
    "    \n",
    "    # Detect people in the frame using HOG\n",
    "    boxes, weights = hog.detectMultiScale(frame, winStride=(4, 4), padding=(8, 8), scale=1.05)\n",
    "    \n",
    "    # Draw bounding boxes around the detected people\n",
    "    for (x, y, w, h) in boxes:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    \n",
    "    # Display the frame with bounding boxes\n",
    "    cv2.imshow('Pose Detection', frame)\n",
    "    \n",
    "    # Exit the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
